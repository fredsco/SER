{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Speech Emotion Recognizer\n",
    "\n",
    "This program uses Python and SciKit learn for a speech emotion recognizer. This programs detects\n",
    "emotion from human speech tone. The model uses the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\n",
    "\n",
    "The whole pipeline is as follows (as same as any machine learning pipeline):\n",
    "\n",
    "\n",
    "1. Preparing the Dataset: Downloading and converting the dataset to be suited for extraction.\n",
    "\n",
    "2. Loading the Dataset: This process is about loading the dataset in Python which involves extracting audio features, such as obtaining different features such as power, pitch and vocal tract configuration from the speech signal,  librosa library is used to do that.\n",
    "\n",
    "3. Training the Model: After the dataset has been prepared and loaded, it is trained on a suited sklearn model.\n",
    "\n",
    "4. Testing the Model: Measuring how good the model is doing.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import soundfile # to read audio file\n",
    "import numpy as np\n",
    "import librosa # to extract speech features\n",
    "import glob\n",
    "import os\n",
    "import pickle # to save model after training\n",
    "from sklearn.model_selection import train_test_split # for splitting training and testing\n",
    "from sklearn.neural_network import MLPClassifier # multi-layer perceptron model\n",
    "from sklearn.metrics import accuracy_score # to measure how good we are\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extraction\n",
    "\n",
    "The following function handles extracting features. This is changing the speech waveform to a form of parametric representation\n",
    "at a relative lesser data rate:\n",
    "\n",
    "(A parametric equation is commonly used to express the coordinates of the points that make up a geometric object such as a curve or surface)\n",
    "\n",
    "MFCC, chroma and spectrogram frequency are used as speech features rather than raw waveform as these might contain unnecessary data\n",
    "that doesn't help on the classificaiton."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc) Mel frequency cepstrum  a representation of the short-term power spectrum of sound\n",
    "            - Chroma (chroma) chroma feature which refers to the twelfe different pitch classes\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz) tonal space\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\") #\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma or contrast:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        if contrast:\n",
    "            contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, contrast))\n",
    "        if tonnetz:\n",
    "            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loading\n",
    "\n",
    "After the extraction we need to load the dataset containing voice samples of different actors.\n",
    "The dataset used (RAVDESS) already contains emotions which will be defined in int2emotion.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "int2emotion = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "\n",
    "# Currently the following will be used for the recognizer for classification. as adding more would mess the accuracy\n",
    "AVAILABLE_EMOTIONS = {\n",
    "    \"angry\",\n",
    "    \"sad\",\n",
    "    \"neutral\",\n",
    "    \"happy\",\n",
    "    # \"surprised\",\n",
    "    # \"fearful\"\n",
    "}\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    X, y = [], []\n",
    "    for file in glob.glob(\"dataset/Actor_*/*.wav\"):\n",
    "        # get the base name of the audio file\n",
    "        basename = os.path.basename(file)\n",
    "        # get the emotion label\n",
    "        emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "        # we allow only AVAILABLE_EMOTIONS we set\n",
    "        if emotion not in AVAILABLE_EMOTIONS:\n",
    "            continue\n",
    "        # extract soeech features\n",
    "        features = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        # add to data\n",
    "        X.append(features)\n",
    "        y.append(emotion)\n",
    "\n",
    "    # split the data to training and testing and return it\n",
    "    return train_test_split(np.array(X), y, test_size=test_size, random_state=7)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can load the dataset with 75% training and 25% testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load RAVDESS dataset, 75% training 25% testing\n",
    "X_train, X_test, y_train, y_test = load_data(test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some logging regarding dataset information:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print some details\n",
    "# number of samples in training data\n",
    "print(\"[+] Number of training samples:\", X_train.shape[0])\n",
    "# number of samples in testing data\n",
    "print(\"[+] Number of testing samples:\", X_test.shape[0])\n",
    "# number of features used\n",
    "# this is a vector of features extracted\n",
    "# using extract_features() function\n",
    "print(\"[+] Number of features:\", X_train.shape[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now a grid search is required on MLPClassifier to get the best possible hyper parameters, this is tweaked based on findings on the internet and what was currently \"ok\" :\n",
    "\n",
    "The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n",
    "\n",
    "In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters are derived via training.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# best model, determined by a grid search\n",
    "\n",
    "model_params = {\n",
    "    'alpha': 0.01,\n",
    "    'batch_size': 256,\n",
    "    'epsilon': 1e-08,\n",
    "    'hidden_layer_sizes': (300,),\n",
    "    'learning_rate': 'adaptive',\n",
    "    'max_iter': 500,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important to mention that this is a fully connected (dense) neural network with one layer that contains 300 units, a btach size of 256, 500 iteration\n",
    "and an adaptive learning rate.\n",
    "\n",
    "Now we can initialize the model with the model_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize Multi Layer Perceptron classifier\n",
    "# with best parameters ( so far )\n",
    "model = MLPClassifier(**model_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After init, we can start training the model with the dataset loaded:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can calculate the accuracy score and print it to meassure how good the model is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict 25% of data to measure how good we are\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last but not least, we save the model!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make result directory if doesn't exist yet\n",
    "if not os.path.isdir(\"result\"):\n",
    "    os.mkdir(\"result\")\n",
    "\n",
    "pickle.dump(model, open(\"result/mlp_classifier.model\", \"wb\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the model!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"result/mlp_classifier.model\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now let's extract features from a wav file and then load these features against the trained model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = extract_feature(\"tests/happybutangry.wav\", mfcc=True, chroma=True, mel=True).reshape(1, -1)\n",
    "result = loaded_model.predict(features)[0]\n",
    "\n",
    "print(\"result:\", result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}